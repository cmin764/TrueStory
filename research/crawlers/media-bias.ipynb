{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AllSides sources & bias crawler\n",
    "\n",
    "Get and save a list of rated news sources as left or right and in between.\n",
    "\n",
    "A CSV file will be created with the following columns:\n",
    "\n",
    "- Source\n",
    "- Label\n",
    "- Agree\n",
    "- Disagree\n",
    "- Publisher URL\n",
    "- Publisher site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b]0;IPython: research/crawlers\u0007Requirement already satisfied: aiohttp in /Users/cmin/anaconda3/lib/python3.7/site-packages (3.6.2)\n",
      "Requirement already satisfied: bs4 in /Users/cmin/anaconda3/lib/python3.7/site-packages (0.0.1)\n",
      "Requirement already satisfied: requests in /Users/cmin/anaconda3/lib/python3.7/site-packages (2.22.0)\n",
      "Requirement already satisfied: async-timeout<4.0,>=3.0 in /Users/cmin/anaconda3/lib/python3.7/site-packages (from aiohttp) (3.0.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/cmin/anaconda3/lib/python3.7/site-packages (from aiohttp) (1.3.0)\n",
      "Requirement already satisfied: chardet<4.0,>=2.0 in /Users/cmin/anaconda3/lib/python3.7/site-packages (from aiohttp) (3.0.4)\n",
      "Requirement already satisfied: multidict<5.0,>=4.5 in /Users/cmin/anaconda3/lib/python3.7/site-packages (from aiohttp) (4.6.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/cmin/anaconda3/lib/python3.7/site-packages (from aiohttp) (19.1.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/cmin/anaconda3/lib/python3.7/site-packages (from bs4) (4.7.1)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /Users/cmin/anaconda3/lib/python3.7/site-packages (from requests) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/cmin/anaconda3/lib/python3.7/site-packages (from requests) (1.24.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/cmin/anaconda3/lib/python3.7/site-packages (from requests) (2019.6.16)\n",
      "Requirement already satisfied: soupsieve>=1.2 in /Users/cmin/anaconda3/lib/python3.7/site-packages (from beautifulsoup4->bs4) (1.8)\n"
     ]
    }
   ],
   "source": [
    "!ipython -m pip install aiohttp bs4 requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import csv\n",
    "import logging\n",
    "import re\n",
    "import urllib.parse as urlparse\n",
    "\n",
    "import aiohttp\n",
    "\n",
    "import bs4\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - root - 2019-11-24 20:49:11,101 - Crawling page 28...\n",
      "DEBUG - root - 2019-11-24 20:49:11,529 - Getting publisher's URL for 'Yahoo! The 360'.\n",
      "DEBUG - root - 2019-11-24 20:49:11,530 - Getting publisher's URL for 'Yes! Magazine'.\n",
      "INFO - root - 2019-11-24 20:49:12,481 - Crawling page 29...\n",
      "INFO - root - 2019-11-24 20:49:12,639 - Reached empty table -> end of results/pages.\n"
     ]
    }
   ],
   "source": [
    "url_tpl = \"https://www.allsides.com/media-bias/media-bias-ratings?field_featured_bias_rating_value=All&field_news_source_type_tid%5B1%5D=1&field_news_source_type_tid%5B2%5D=2&field_news_source_type_tid%5B3%5D=3&field_news_bias_nid_1%5B1%5D=1&field_news_bias_nid_1%5B2%5D=2&field_news_bias_nid_1%5B3%5D=3&title=&customFilter=1&page={}\"\n",
    "html_parser = \"html5lib\"\n",
    "csv_header = [\n",
    "    \"source\",\n",
    "    \"label\",\n",
    "    \"agree\",\n",
    "    \"disagree\",\n",
    "    \"publisher\",\n",
    "    \"site\",\n",
    "]\n",
    "dump_path = \"media-bias.csv\"\n",
    "encoding = \"utf-8\"\n",
    "skip_blocked_sites = True\n",
    "\n",
    "verbose = True  # make it True to see debugging messages\n",
    "level = logging.DEBUG if verbose else logging.INFO\n",
    "logging.root.handlers.clear()\n",
    "logging.basicConfig(\n",
    "    format=\"%(levelname)s - %(name)s - %(asctime)s - %(message)s\",\n",
    "    level=level\n",
    ")\n",
    "\n",
    "\n",
    "async def get_soup(session, url):\n",
    "    abs_url = urlparse.urljoin(url_tpl, url)\n",
    "    text = await (await session.get(abs_url)).text()\n",
    "#     resp.raise_for_status()\n",
    "    soup = bs4.BeautifulSoup(text, html_parser)\n",
    "    return soup\n",
    "\n",
    "\n",
    "async def get_publisher_url(session, src_url, source_name):\n",
    "#     import code; code.interact(local={**globals(), **locals()})\n",
    "    logging.debug(\"Getting publisher's URL for %r.\", source_name)\n",
    "    soup = await get_soup(session, src_url)\n",
    "    div = soup.find(\"div\", class_=\"dynamic-grid\")\n",
    "    if not div:\n",
    "        return None\n",
    "    \n",
    "    url = div.find(\"a\").get(\"href\").strip()\n",
    "    parsed = urlparse.urlparse(url)\n",
    "    if not parsed.netloc:\n",
    "        return None\n",
    "    \n",
    "    return url, parsed.netloc\n",
    "\n",
    "\n",
    "async def save_pages(bias_writer, csvfile):\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        page = 28  # custom page if you want\n",
    "        while True:\n",
    "            logging.info(\"Crawling page %d...\", page)\n",
    "            url = url_tpl.format(page)\n",
    "            soup = await get_soup(session, url)\n",
    "            \n",
    "            pub_coros = []\n",
    "            extras = []\n",
    "            table = soup.find(\"table\")\n",
    "            if not table or \"no record\" in table.find(\"tbody\").find(\"tr\").text.lower():\n",
    "                logging.info(\"Reached empty table -> end of results/pages.\")\n",
    "                break\n",
    "                \n",
    "            for row in table.find(\"tbody\").find_all(\"tr\"):\n",
    "                src_a = row.find(\"td\", class_=\"source-title\").find(\"a\")\n",
    "                src_url = src_a.get(\"href\")\n",
    "                source_name = src_a.text\n",
    "                label_alt = row.find(\"td\", class_=\"views-field-field-bias-image\").find(\"img\").get(\"alt\")\n",
    "                label = label_alt.split(\":\")[-1].strip()\n",
    "                feedback = row.find(\"td\", class_=\"community-feedback\")\n",
    "                agree = int(feedback.find(\"span\", class_=\"agree\").text)\n",
    "                disagree = int(feedback.find(\"span\", class_=\"disagree\").text)\n",
    "                \n",
    "                extras.append([source_name, label, agree, disagree])\n",
    "#                 import code; code.interact(local={**globals(), **locals()})\n",
    "                pub_coros.append(get_publisher_url(session, src_url, source_name))\n",
    "            \n",
    "            publisher_details_list = await asyncio.gather(*pub_coros)\n",
    "            for idx, publisher_details in enumerate(publisher_details_list):\n",
    "                if not publisher_details:\n",
    "                    if skip_blocked_sites:\n",
    "                        continue\n",
    "                    else:\n",
    "                        publisher_details = (\"\", \"\")\n",
    "\n",
    "#                 print(source_name, label, f\"{agree}/{disagree}\")\n",
    "                bias_writer.writerow(extras[idx] + list(publisher_details))\n",
    "\n",
    "            page += 1\n",
    "            csvfile.flush()\n",
    "\n",
    "\n",
    "async def main():\n",
    "    with open(dump_path, \"w\", newline=\"\", encoding=encoding) as csvfile:\n",
    "        bias_writer = csv.writer(csvfile)\n",
    "        bias_writer.writerow(csv_header)\n",
    "        await save_pages(bias_writer, csvfile)\n",
    "        \n",
    "        \n",
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some publishers are blocked (no websites offered by AllSides), therefore fewer results in the CSV file.\n",
    "\n",
    "Now let's find a good way of associating a side with a website in case multiple candidates are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "side_dict = {}\n",
    "\n",
    "with open(dump_path, newline=\"\") as stream:\n",
    "    reader = csv.reader(stream)\n",
    "    print(next(reader))\n",
    "    \n",
    "    for row in reader:\n",
    "        side_dict.setdefault(row[5], []).append((row[0], row[1], row[2]))\n",
    "\n",
    "for site, sides in side_dict.items():\n",
    "    if len(sides) > 1:\n",
    "        print(site, sides)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
